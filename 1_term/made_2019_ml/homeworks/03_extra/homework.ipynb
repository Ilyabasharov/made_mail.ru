{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 extra: texts clastering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jsonlines\n",
    "import json\n",
    "import tldextract\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ilyabasharov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ilyabasharov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month(code):\n",
    "    \n",
    "    if code == 1:\n",
    "        return 'январь'\n",
    "    elif code == 2:\n",
    "        return 'февраль'\n",
    "    elif code == 3:\n",
    "        return 'март'\n",
    "    elif code == 4:\n",
    "        return 'апрель'\n",
    "    elif code == 5:\n",
    "        return 'май'\n",
    "    elif code == 6:\n",
    "        return 'июнь'\n",
    "    elif code == 7:\n",
    "        return 'июль'\n",
    "    elif code == 8:\n",
    "        return 'август'\n",
    "    elif code == 9:\n",
    "        return 'сентябрь'\n",
    "    elif code == 10:\n",
    "        return 'октябрь'\n",
    "    elif code == 11:\n",
    "        return 'ноябрь'\n",
    "    else:\n",
    "        return 'декабрь'\n",
    "    \n",
    "def day(code):\n",
    "    \n",
    "    if code == 0:\n",
    "        return 'понедельник'\n",
    "    elif code == 1:\n",
    "        return 'вторник'\n",
    "    elif code == 2:\n",
    "        return 'среда'\n",
    "    elif code == 3:\n",
    "        return 'четверг'\n",
    "    elif code == 4:\n",
    "        return 'пятница'\n",
    "    elif code == 5:\n",
    "        return 'суббота'\n",
    "    else:\n",
    "        return 'воскресенье'\n",
    "\n",
    "def year(code):\n",
    "    \n",
    "    return str(code)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(file_jsonl, file_json):\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/russian.pickle')\n",
    "    russian_stopwords = nltk.corpus.stopwords.words('russian')\n",
    "    morph = Mystem()\n",
    "    \n",
    "    with open(file_json) as file:\n",
    "        doc_ids = json.load(file)\n",
    "    \n",
    "    train, ans, test, dc_ids = [], [], [], []\n",
    "    \n",
    "    with jsonlines.open(file_jsonl) as reader:\n",
    "        for obj in tqdm(reader):\n",
    "            \n",
    "            sentanses = tokenizer.tokenize((obj['description'] + obj['title']).lower().strip())\n",
    "            sentanses = sum([x.split(';') for x in sentanses], [])\n",
    "            sentanses = [morph.lemmatize(word) for word in sentanses]\n",
    "            sentanses = [[y for y in x if re.match('[а-яёa-z0-9]', y) and y not in russian_stopwords] for x in sentanses]\n",
    "            sentanses = sum(sentanses, []) + \\\n",
    "                        [tldextract.extract(obj['url']).domain] + \\\n",
    "                        [month(time.localtime(obj['ts']).tm_mon)] + \\\n",
    "                        [day(time.localtime(obj['ts']).tm_wday)] + \\\n",
    "                        [year(time.localtime(obj['ts']).tm_year)] + \\\n",
    "                        [year(time.localtime(obj['ts']).tm_mday)]\n",
    "            \n",
    "            if str(obj['doc_id']) in doc_ids:\n",
    "                train.append(' '.join(sentanses))\n",
    "                ans.append(doc_ids[str(obj['doc_id'])])\n",
    "            else:\n",
    "                test.append(' '.join(sentanses))\n",
    "                dc_ids.append(obj['doc_id'])\n",
    "            \n",
    "    return train, ans, test, dc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88368it [06:28, 227.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train, ans, test, dc_ids = create_data('cosmo_content_storage_final_cut.jsonl', 'cluster_final_cut_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26510 26510 61858\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(ans), len(test))\n",
    "assert len(ans) == len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, ans, test_size=0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('vect', CountVectorizer(max_features = 20000)),\n",
    "                     ('tfidf', TfidfTransformer())\n",
    "                    ])\n",
    "pipeline.fit(all_data)\n",
    "\n",
    "X_train = pipeline.transform(X_train)\n",
    "X_test = pipeline.transform(X_test)\n",
    "Result = pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23859, 20000), (2651, 20000), (61858, 20000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, Result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=50, tol=1e-5, n_jobs = -1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(penalty = 'l2', solver = 'lbfgs', n_jobs=4, C=1e4, multi_class = 'auto')\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = logreg.predict(Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape, len(dc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('solution.txt', 'w') as fout:\n",
    "    print(\"doc_id,cat\", file=fout)\n",
    "    for index, dc_id in enumerate(dc_ids):\n",
    "        print(dc_id, result[index], sep=',', file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
